{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "185f840b-19ab-425c-8ed6-528225528d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: lxml in c:\\programdata\\anaconda3\\lib\\site-packages (5.3.0)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.7-cp313-cp313-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: seaborn in c:\\programdata\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Collecting textblob\n",
      "  Downloading textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2025.4.26)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.13-cp313-cp313-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp313-cp313-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.10-cp313-cp313-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.6-cp313-cp313-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp313-cp313-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.10.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (72.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (24.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Using cached langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Using cached language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.0-cp313-cp313-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached cloudpathlib-0.23.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.4.2-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: wrapt in c:\\programdata\\anaconda3\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.3.1-cp313-cp313-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Downloading spacy-3.8.7-cp313-cp313-win_amd64.whl (13.9 MB)\n",
      "   ---------------------------------------- 0.0/13.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/13.9 MB 4.9 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.4/13.9 MB 7.6 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 3.4/13.9 MB 6.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 5.0/13.9 MB 6.5 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 6.3/13.9 MB 7.1 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 7.1/13.9 MB 6.1 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 7.3/13.9 MB 5.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 8.1/13.9 MB 5.2 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 8.7/13.9 MB 5.0 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 9.2/13.9 MB 4.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 9.4/13.9 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 10.0/13.9 MB 4.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 10.2/13.9 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 10.5/13.9 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 11.0/13.9 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 11.3/13.9 MB 3.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 11.8/13.9 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 12.1/13.9 MB 3.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 12.3/13.9 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.6/13.9 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.8/13.9 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 13.1/13.9 MB 2.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.4/13.9 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.6/13.9 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.9/13.9 MB 2.7 MB/s eta 0:00:00\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp313-cp313-win_amd64.whl (39 kB)\n",
      "Using cached langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.13-cp313-cp313-win_amd64.whl (24 kB)\n",
      "Downloading preshed-3.0.10-cp313-cp313-win_amd64.whl (115 kB)\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp313-cp313-win_amd64.whl (630 kB)\n",
      "   ---------------------------------------- 0.0/630.6 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 262.1/630.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 630.6/630.6 kB 1.7 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.6-cp313-cp313-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.7 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 0.5/1.7 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.0/1.7 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.3/1.7 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading blis-1.3.0-cp313-cp313-win_amd64.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.3 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.8/6.3 MB 2.3 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.0/6.3 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 1.6/6.3 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 1.8/6.3 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.1/6.3 MB 1.8 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 2.6/6.3 MB 1.8 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 3.1/6.3 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 3.4/6.3 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 3.9/6.3 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 4.2/6.3 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 4.7/6.3 MB 2.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.2/6.3 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.8/6.3 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 2.0 MB/s eta 0:00:00\n",
      "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Using cached cloudpathlib-0.23.0-py3-none-any.whl (62 kB)\n",
      "Downloading smart_open-7.4.2-py3-none-any.whl (63 kB)\n",
      "Downloading textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "   ---------------------------------------- 0.0/624.3 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 262.1/624.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 624.3/624.3 kB 2.2 MB/s eta 0:00:00\n",
      "Using cached language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "Downloading marisa_trie-1.3.1-cp313-cp313-win_amd64.whl (139 kB)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, smart-open, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, srsly, preshed, language-data, textblob, langcodes, confection, weasel, thinc, spacy\n",
      "\n",
      "   ------ ---------------------------------  3/19 [spacy-legacy]\n",
      "   -------- -------------------------------  4/19 [smart-open]\n",
      "   ------------ ---------------------------  6/19 [marisa-trie]\n",
      "   -------------- -------------------------  7/19 [cloudpathlib]\n",
      "   ------------------ ---------------------  9/19 [blis]\n",
      "   --------------------- ------------------ 10/19 [srsly]\n",
      "   --------------------- ------------------ 10/19 [srsly]\n",
      "   --------------------- ------------------ 10/19 [srsly]\n",
      "   --------------------- ------------------ 10/19 [srsly]\n",
      "   ------------------------- -------------- 12/19 [language-data]\n",
      "   ------------------------- -------------- 12/19 [language-data]\n",
      "   ------------------------- -------------- 12/19 [language-data]\n",
      "   ------------------------- -------------- 12/19 [language-data]\n",
      "   ------------------------- -------------- 12/19 [language-data]\n",
      "   --------------------------- ------------ 13/19 [textblob]\n",
      "   ------------------------------- -------- 15/19 [confection]\n",
      "   --------------------------------- ------ 16/19 [weasel]\n",
      "   ----------------------------------- ---- 17/19 [thinc]\n",
      "   ----------------------------------- ---- 17/19 [thinc]\n",
      "   ----------------------------------- ---- 17/19 [thinc]\n",
      "   ----------------------------------- ---- 17/19 [thinc]\n",
      "   ------------------------------------- -- 18/19 [spacy]\n",
      "   ------------------------------------- -- 18/19 [spacy]\n",
      "   ------------------------------------- -- 18/19 [spacy]\n",
      "   ------------------------------------- -- 18/19 [spacy]\n",
      "   ------------------------------------- -- 18/19 [spacy]\n",
      "   ------------------------------------- -- 18/19 [spacy]\n",
      "   ------------------------------------- -- 18/19 [spacy]\n",
      "   ------------------------------------- -- 18/19 [spacy]\n",
      "   ------------------------------------- -- 18/19 [spacy]\n",
      "   ------------------------------------- -- 18/19 [spacy]\n",
      "   ------------------------------------- -- 18/19 [spacy]\n",
      "   ------------------------------------- -- 18/19 [spacy]\n",
      "   ------------------------------------- -- 18/19 [spacy]\n",
      "   ------------------------------------- -- 18/19 [spacy]\n",
      "   ------------------------------------- -- 18/19 [spacy]\n",
      "   ------------------------------------- -- 18/19 [spacy]\n",
      "   ------------------------------------- -- 18/19 [spacy]\n",
      "   ------------------------------------- -- 18/19 [spacy]\n",
      "   ------------------------------------- -- 18/19 [spacy]\n",
      "   ------------------------------------- -- 18/19 [spacy]\n",
      "   ------------------------------------- -- 18/19 [spacy]\n",
      "   ------------------------------------- -- 18/19 [spacy]\n",
      "   ---------------------------------------- 19/19 [spacy]\n",
      "\n",
      "Successfully installed blis-1.3.0 catalogue-2.0.10 cloudpathlib-0.23.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.3.1 murmurhash-1.0.13 preshed-3.0.10 smart-open-7.4.2 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 textblob-0.19.0 thinc-8.3.6 wasabi-1.1.3 weasel-0.4.1\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/12.8 MB 5.4 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 2.4/12.8 MB 7.4 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 6.7 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 5.0/12.8 MB 6.7 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 6.3/12.8 MB 6.9 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 6.8/12.8 MB 6.0 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 7.9/12.8 MB 5.1 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.7/12.8 MB 4.9 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 4.7 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.7/12.8 MB 4.5 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.2/12.8 MB 4.3 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 4.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.3/12.8 MB 4.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.8/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 3.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 3.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 3.7 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Installation of required libraries\n",
    "!pip install requests beautifulsoup4 lxml pandas spacy matplotlib seaborn textblob\n",
    "# Download the small English language model for spaCy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5ff3af8-63d5-4be0-ba55-ab6101bf731a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ead1df5d-a8d1-4442-aacd-d419e1bc777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46cc4dbe-45ce-468c-bb57-048207506a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 10000000\n",
    "data = np.random.rand(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c80652a-55e4-46ce-a64b-93fba6233a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy vectorized sum\n",
    "start_np = time.time()\n",
    "np_sum = np.sum(data)\n",
    "end_np = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d742ca33-365c-4330-96ed-f8c3e062c03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Python loop sum\n",
    "start_py = time.time()\n",
    "manual_sum = 0\n",
    "for val in data:\n",
    " manual_sum += val\n",
    "end_py = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c0be6fb-95e8-442c-b1b9-96c753e2aa4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Sum: 5000608.12 | Time: 0.02365 sec\n",
      "Manual Sum: 5000608.12 | Time: 4.66086 sec\n"
     ]
    }
   ],
   "source": [
    "print(f\"NumPy Sum: {np_sum:.2f} | Time: {end_np - start_np:.5f} sec\")\n",
    "print(f\"Manual Sum: {manual_sum:.2f} | Time: {end_py - start_py:.5f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fc3f84d-95f1-4cdd-9d37-b05db0d221ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected 'except' or 'finally' block (2221495418.py, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[15], line 19\u001b[1;36m\u001b[0m\n\u001b[1;33m    soup = BeautifulSoup(res.content, 'lxml')\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m expected 'except' or 'finally' block\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "urls = [\n",
    " 'https://techncruncher.blogspot.com/2025/01/top-10-ai-tools-that-will-transform.html',\n",
    " 'https://techncruncher.blogspot.com/2023/12/limewire-ai-studio-review-2023-details.html',\n",
    " 'https://techncruncher.blogspot.com/2023/01/top-10-ai-tools-in-2023-that-will-make.html',\n",
    " 'https://techncruncher.blogspot.com/2022/11/top-10-ai-content-generator-writer.html',\n",
    " 'https://techncruncher.blogspot.com/2022/09/cj-affiliate-ultimate-guide-to.html'\n",
    "]\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
    "articles = []\n",
    "for url in urls:\n",
    " try:\n",
    " # Fetch the page content\n",
    "     res = requests.get(url, headers=headers, timeout=10)\n",
    "     res.raise_for_status()\n",
    " soup = BeautifulSoup(res.content, 'lxml')\n",
    " # Extract Title\n",
    " title = soup.find('h3', class_='post-title')\n",
    " title = title.get_text(strip=True) if title else soup.title.get_text(strip=True)\n",
    " # Extract Body Text\n",
    " content_div = soup.find('div', class_='post-body entry-content')\n",
    " if not content_div:\n",
    " continue\n",
    " paragraphs = content_div.find_all('p')\n",
    " text = ' '.join(p.get_text(strip=True) for p in paragraphs)\n",
    " # Skip short articles for quality control\n",
    " if len(text) > 100:\n",
    " articles.append({'url': url, 'title': title, 'text': text})\n",
    " else:\n",
    " print(f\"[SKIP] Too short: {url}\")\n",
    " time.sleep(1) # Be polite to the server\n",
    " except Exception as e:\n",
    " print(f\"[ERROR] {url} -> {e}\")\n",
    "df = pd.DataFrame(articles)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1ece94b-97fd-483c-8ffd-5f64336ee23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "urls = [\n",
    "    'https://www.bbc.com/urdu'\n",
    "]\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
    "articles = []\n",
    "\n",
    "for url in urls:\n",
    "    try:\n",
    "        # Fetch the page content\n",
    "        res = requests.get(url, headers=headers, timeout=10)\n",
    "        res.raise_for_status()\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "\n",
    "        # Extract Title\n",
    "        title = soup.find('h3', class_='post-title')\n",
    "        title = title.get_text(strip=True) if title else soup.title.get_text(strip=True)\n",
    "\n",
    "        # Extract Body Text\n",
    "        content_div = soup.find('div', class_='post-body entry-content')\n",
    "        if not content_div:\n",
    "            continue\n",
    "\n",
    "        paragraphs = content_div.find_all('p')\n",
    "        text = ' '.join(p.get_text(strip=True) for p in paragraphs)\n",
    "\n",
    "        # Skip short articles for quality control\n",
    "        if len(text) > 100:\n",
    "            articles.append({'url': url, 'title': title, 'text': text})\n",
    "        else:\n",
    "            print(f\"[SKIP] Too short: {url}\")\n",
    "\n",
    "        time.sleep(1)  # Be polite to the server\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {url} -> {e}\")\n",
    "\n",
    "# Save results to a DataFrame\n",
    "df = pd.DataFrame(articles)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "231758e3-7b28-459c-b893-4fb3e218797b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "urls = [\n",
    "    'https://www.bbc.com/urdu'\n",
    "]\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
    "articles = []\n",
    "\n",
    "for url in urls:\n",
    "    try:\n",
    "        # Fetch the page content\n",
    "        res = requests.get(url, headers=headers, timeout=10)\n",
    "        res.raise_for_status()\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "\n",
    "        # Extract Title\n",
    "        title = soup.find('h3', class_='post-title')\n",
    "        title = title.get_text(strip=True) if title else soup.title.get_text(strip=True)\n",
    "\n",
    "        # Extract Body Text\n",
    "        content_div = soup.find('div', class_='post-body entry-content')\n",
    "        if not content_div:\n",
    "            continue\n",
    "\n",
    "        paragraphs = content_div.find_all('p')\n",
    "        text = ' '.join(p.get_text(strip=True) for p in paragraphs)\n",
    "\n",
    "        # Skip short articles for quality control\n",
    "        if len(text) > 100:\n",
    "            articles.append({'url': url, 'title': title, 'text': text})\n",
    "        else:\n",
    "            print(f\"[SKIP] Too short: {url}\")\n",
    "\n",
    "        time.sleep(1)  # Be polite to the server\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {url} -> {e}\")\n",
    "\n",
    "# Save results to a DataFrame\n",
    "df = pd.DataFrame(articles)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d6a9f69-1694-4329-9e36-a33c0f3cca11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] صدر ٹرمپ کا پاکستان سمیت دیگر ممالک پر جوہری تجربے...\n",
      "[ERROR] https://www.bbc.com/urdu/articles/cp9779l57kyo -> HTTPSConnectionPool(host='www.bbc.com', port=443): Max retries exceeded with url: /urdu/articles/cp9779l57kyo (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001B6B28F91D0>, 'Connection to www.bbc.com timed out. (connect timeout=10)'))\n",
      "[OK] ’گریٹر افغانستان‘ کا متنازع نقشہ: ’اس قسم کی اشتعا...\n",
      "[OK] پائیکا: ’میرا ایک سال کا بچہ کتابیں، کھلونے، فرنیچ...\n",
      "[OK] انڈین خواتین ٹیم کی ورلڈ کپ میں تاریخی فتح، جے شاہ...\n",
      "[OK] موسمیاتی تبدیلی پاکستان میں سیلابی صورت حال کو کس ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>article</th>\n",
       "      <th>genre</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>صدر ٹرمپ کا پاکستان سمیت دیگر ممالک پر جوہری ت...</td>\n",
       "      <td>،تصویر کا ذریعہGetty Images امریکہ کے صدر ڈونل...</td>\n",
       "      <td>BBC News,اردو</td>\n",
       "      <td>https://www.bbc.com/urdu/articles/clyzddk4jgeo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>’گریٹر افغانستان‘ کا متنازع نقشہ: ’اس قسم کی ا...</td>\n",
       "      <td>،تصویر کا ذریعہX/@SamiYousafzaii پاکستان اور ا...</td>\n",
       "      <td>BBC News,اردو</td>\n",
       "      <td>https://www.bbc.com/urdu/articles/c1k0pww2vy1o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>پائیکا: ’میرا ایک سال کا بچہ کتابیں، کھلونے، ف...</td>\n",
       "      <td>،تصویر کا ذریعہCraig Colville جب چھوٹے بچے چلن...</td>\n",
       "      <td>BBC News,اردو</td>\n",
       "      <td>https://www.bbc.com/urdu/articles/cly95821rv8o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>انڈین خواتین ٹیم کی ورلڈ کپ میں تاریخی فتح، جے...</td>\n",
       "      <td>،تصویر کا ذریعہBCCI انڈین خواتین کرکٹ ٹیم کے ل...</td>\n",
       "      <td>BBC News,اردو</td>\n",
       "      <td>https://www.bbc.com/urdu/articles/cvgwjr2ney8o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>موسمیاتی تبدیلی پاکستان میں سیلابی صورت حال کو...</td>\n",
       "      <td>امدادی کارکنوں اور رشتہ داروں نے ایک سالہ زارا...</td>\n",
       "      <td>BBC News,اردو</td>\n",
       "      <td>https://www.bbc.com/urdu/articles/c5ypp1278j4o</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                           headline  \\\n",
       "0  2025-11-03  صدر ٹرمپ کا پاکستان سمیت دیگر ممالک پر جوہری ت...   \n",
       "1  2025-11-03  ’گریٹر افغانستان‘ کا متنازع نقشہ: ’اس قسم کی ا...   \n",
       "2  2025-11-03  پائیکا: ’میرا ایک سال کا بچہ کتابیں، کھلونے، ف...   \n",
       "3  2025-11-03  انڈین خواتین ٹیم کی ورلڈ کپ میں تاریخی فتح، جے...   \n",
       "4  2025-11-03  موسمیاتی تبدیلی پاکستان میں سیلابی صورت حال کو...   \n",
       "\n",
       "                                             article          genre  \\\n",
       "0  ،تصویر کا ذریعہGetty Images امریکہ کے صدر ڈونل...  BBC News,اردو   \n",
       "1  ،تصویر کا ذریعہX/@SamiYousafzaii پاکستان اور ا...  BBC News,اردو   \n",
       "2  ،تصویر کا ذریعہCraig Colville جب چھوٹے بچے چلن...  BBC News,اردو   \n",
       "3  ،تصویر کا ذریعہBCCI انڈین خواتین کرکٹ ٹیم کے ل...  BBC News,اردو   \n",
       "4  امدادی کارکنوں اور رشتہ داروں نے ایک سالہ زارا...  BBC News,اردو   \n",
       "\n",
       "                                              url  \n",
       "0  https://www.bbc.com/urdu/articles/clyzddk4jgeo  \n",
       "1  https://www.bbc.com/urdu/articles/c1k0pww2vy1o  \n",
       "2  https://www.bbc.com/urdu/articles/cly95821rv8o  \n",
       "3  https://www.bbc.com/urdu/articles/cvgwjr2ney8o  \n",
       "4  https://www.bbc.com/urdu/articles/c5ypp1278j4o  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# BBC Urdu homepage\n",
    "url = \"https://www.bbc.com/urdu\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
    "\n",
    "res = requests.get(url, headers=headers)\n",
    "res.raise_for_status()\n",
    "soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "articles = []\n",
    "\n",
    "# Step 1: Find all article blocks\n",
    "# (BBC uses <a> tags inside divs with data-testid='internal-link')\n",
    "for link in soup.find_all('a', href=True):\n",
    "    href = link['href']\n",
    "    # BBC Urdu articles often have '/urdu/articles/' in URL\n",
    "    if '/urdu/articles/' in href:\n",
    "        article_url = \"https://www.bbc.com\" + href if href.startswith('/') else href\n",
    "        try:\n",
    "            # Step 2: Visit each article link\n",
    "            r = requests.get(article_url, headers=headers, timeout=10)\n",
    "            r.raise_for_status()\n",
    "            article_soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "            # Extract title\n",
    "            title_tag = article_soup.find(\"h1\")\n",
    "            title = title_tag.get_text(strip=True) if title_tag else \"No title\"\n",
    "\n",
    "            # Extract paragraphs (article text)\n",
    "            paragraphs = [p.get_text(strip=True) for p in article_soup.find_all(\"p\")]\n",
    "            text = \" \".join(paragraphs[:10])  # first few paragraphs\n",
    "\n",
    "            # Extract date (from <time> tag if available)\n",
    "            time_tag = article_soup.find(\"time\")\n",
    "            date = time_tag.get(\"datetime\") if time_tag and time_tag.has_attr(\"datetime\") else \"Unknown\"\n",
    "\n",
    "            # Extract genre (BBC shows it inside breadcrumb links)\n",
    "            genre_tag = article_soup.find(\"a\", {\"class\": lambda x: x and \"bbc\" in x})\n",
    "            genre = genre_tag.get_text(strip=True) if genre_tag else \"General\"\n",
    "\n",
    "            # Save data\n",
    "            articles.append({\n",
    "                \"date\": date,\n",
    "                \"headline\": title,\n",
    "                \"article\": text,\n",
    "                \"genre\": genre,\n",
    "                \"url\": article_url\n",
    "            })\n",
    "\n",
    "            print(f\"[OK] {title[:50]}...\")\n",
    "            time.sleep(1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {article_url} -> {e}\")\n",
    "        # Stop after 5 articles just to keep it light\n",
    "        if len(articles) >= 5:\n",
    "            break\n",
    "\n",
    "# Step 3: Make DataFrame\n",
    "df = pd.DataFrame(articles, columns=[\"date\", \"headline\", \"article\", \"genre\", \"url\"])\n",
    "from IPython.display import display\n",
    "\n",
    "display(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c9a25d-0b10-4214-a8c4-d35e821bcd97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
